{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3022: Intro to Data Science - Spring 2020 Practicum 2\n",
    "***\n",
    "\n",
    "This practicum is due on Canvas by **11:59 PM on Friday May 1**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "1. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distribution function or how to perform a particular operation in Python/Pandas. \n",
    "1. This is meant to be like a coding portion of your midterm exam. So, the instructional team will be much less helpful than we typically are with homework. For example, we will not check answers, help debug your code, and so on.\n",
    "1. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "2. You may **NOT** post to message boards or other online resources asking for help.\n",
    "3. You may **NOT** copy-paste solutions *from anywhere*.\n",
    "4. You may **NOT** collaborate with classmates or anyone else.\n",
    "5. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on this practicum or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**Name**: Jeremy M. Hein \n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the practicums nor can you drop your practicum grades. \n",
    "- If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a Practicum clarifications thread. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from math import isnan\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "from calendar import month_name, different_locale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### [50 points] Problem 1: Multiple Linear Regression to Explain House Hauntings\n",
    "\n",
    "<img src=\"https://s-media-cache-ak0.pinimg.com/originals/09/72/01/09720128cff5de4d4af038cd3fcf7f69.jpg\" style=\"width: 300px;\"/>\n",
    "\n",
    "In an effort to control the skyrocketing prices of real estate in the Colorado Front Range, Governor Polis implemented a cutting edge new intervention. This new program oversaw the introduction of ghosts back into their natural ecosystem, after the ghost population seriously dwindled in recent decades due to overhaunting. However, an unfortunate miscalculation has led to haunted houses becoming a very serious problem in Colorado. Modern problems require modern solutions, so Governor Polis has hired you and the famous hedgehog data scientist/part-time ghostbuster Amy to determine what features of a house may be used to best predict a `haunted` score, related to the probability that a house with the given features is haunted (higher $\\leftrightarrow$ more likely to be haunted).\n",
    "\n",
    "You decide to use multiple linear regression to understand and predict what factors lead to increased haunted house hazard. You collected a data set from Haunted Zillow, the lesser-known database of haunted house prices and attributes. The data cover a variety of potential features, and you'll find this data in the file `houses.csv`. \n",
    "\n",
    "**Response**: \n",
    "\n",
    "- $\\texttt{haunted}$: a haunting score, related to the probability that a house with the given features is haunted (higher $\\leftrightarrow$ more likely to be haunted)\n",
    "\n",
    "**Features**: \n",
    "\n",
    "- $\\texttt{age}$: age of the house, in years\n",
    "- $\\texttt{area}$: square footage of interior of house\n",
    "- $\\texttt{bathrooms}$: number of bathrooms\n",
    "- $\\texttt{distance metro}$: distance to the nearest major metropolitan area (in miles)\n",
    "- $\\texttt{distance cemetery}$: distance to the nearest cemetery (in miles)\n",
    "- $\\texttt{cats}$: the number of cats within a one-block radius of the house\n",
    "- $\\texttt{howls}$: the number of wolf howls heard on an average night in the house's neighborhood\n",
    "- $\\texttt{clouds}$: what percentage of the sky was covered by clouds (fraction, 0-1)\n",
    "- $\\texttt{precipitation}$: amount of precipitation in the past 72 hours (inches)\n",
    "- $\\texttt{misery index}$: an economic indicator for how miserable the average United States citizen is, based on the unemployment rate and the inflation rate. More [here](https://www.stuffyoushouldknow.com/podcasts/whats-the-misery-index.htm) and [here](https://en.wikipedia.org/wiki/Misery_index_(economics)). Higher values correspond to more miserable citizens.\n",
    "- $\\texttt{ice cream sold}$: the number of units of ice cream sold at the farmer's market the week the house was most recently sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Read the data from `houses.csv` into a Pandas DataFrame.  Note that since we will be doing a multiple linear regression we will need all of the features, so you should drop any row in the DataFrame that is missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>distance metro</th>\n",
       "      <th>distance cemetery</th>\n",
       "      <th>cats</th>\n",
       "      <th>howls</th>\n",
       "      <th>clouds</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>misery index</th>\n",
       "      <th>ice cream sold</th>\n",
       "      <th>haunted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>118.94</td>\n",
       "      <td>2113.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.23</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>12.99</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.123131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>16.77</td>\n",
       "      <td>184.0</td>\n",
       "      <td>-0.200243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>260.84</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>16.49</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.258651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>101.33</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4.05</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.92</td>\n",
       "      <td>8.28</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.147983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>96.94</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>5.90</td>\n",
       "      <td>178.0</td>\n",
       "      <td>-0.113756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>237.66</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.97</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.16</td>\n",
       "      <td>6.85</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.352572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>65.39</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.59</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.67</td>\n",
       "      <td>16.83</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.331913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>56.24</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>6.42</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.627473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.87</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.96</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.46</td>\n",
       "      <td>19.44</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.411308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>81.79</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.74</td>\n",
       "      <td>9.41</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-0.024012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age    area  bathrooms  distance metro  distance cemetery  cats  howls  \\\n",
       "0   118.94  2113.0          4             7.1               3.23   7.0    3.0   \n",
       "1    20.60  1773.0          2             7.4               4.19   5.0    5.0   \n",
       "2   260.84  2511.0          3             7.0               3.38   2.0    0.0   \n",
       "3   101.33  2000.0          2             7.9               4.05   6.0    8.0   \n",
       "4    96.94  1476.0          1             7.5               5.75   4.0    1.0   \n",
       "..     ...     ...        ...             ...                ...   ...    ...   \n",
       "67  237.66  1403.0          2             6.6               5.97   8.0    2.0   \n",
       "68   65.39  1880.0          4             4.1               3.59   7.0    2.0   \n",
       "69   56.24  1996.0          2             7.4               3.38   4.0    4.0   \n",
       "70    4.87  1732.0          1             7.1               3.96   7.0    2.0   \n",
       "71   81.79  1200.0          2             7.3               3.89   9.0    3.0   \n",
       "\n",
       "    clouds  precipitation  misery index  ice cream sold   haunted  \n",
       "0     1.00           0.82         12.99           273.0  0.123131  \n",
       "1     1.00           0.99         16.77           184.0 -0.200243  \n",
       "2     1.00           1.17         16.49           141.0  0.258651  \n",
       "3     0.13           0.92          8.28           146.0  0.147983  \n",
       "4     1.00           1.73          5.90           178.0 -0.113756  \n",
       "..     ...            ...           ...             ...       ...  \n",
       "67    1.00           2.16          6.85           271.0  0.352572  \n",
       "68    0.60           1.67         16.83           211.0  0.331913  \n",
       "69    1.00           0.71          6.42           215.0  0.627473  \n",
       "70    1.00           1.46         19.44           122.0  0.411308  \n",
       "71    0.88           0.74          9.41           152.0 -0.024012  \n",
       "\n",
       "[72 rows x 12 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load file into pandas dataframe\n",
    "file_path = \"houses.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#drop any rows with missing data\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "#print datafram\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Perform the appropriate statistical test at the $\\alpha = 0.01$ significance level to determine if _at least one_ of the features is related to the the response $y$.  Clearly describe your methodology and show all computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Setup:**\n",
    "\n",
    "Since we're testing if there is at least one feature realted to the response in y, we will use the F-test for MLR.  (Note that if we were to test each feature individually, we would run into the Problem of Multiple Comparisons. Thus, the F-test is appropriate here).  Our hypotheses are as follows:\n",
    "\n",
    "$H_{0}: \\beta_{0} = \\beta_{1} = \\beta_{2} = .... = \\beta_{p} = 0$ (None of our features are related to to the haunted score)\n",
    "\n",
    "$H_{1}: \\beta_{k} \\neq 0$ for at least one value of k in 1, 2, 3, ... , p (At least one feature is realted to the haunted score)\n",
    "\n",
    "Consistent with the F-test, if our F-statistic is greater than 1 and our p-value is less than alpha (0.01, in this case), we can conlude there is sufficient evidence that at least one feature is related to our haunted score and reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerem\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>haunted</td>     <th>  R-squared:         </th> <td>   0.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   16.22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 01 May 2020</td> <th>  Prob (F-statistic):</th> <td>3.75e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:19:42</td>     <th>  Log-Likelihood:    </th> <td>  4.2800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    72</td>      <th>  AIC:               </th> <td>   15.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    60</td>      <th>  BIC:               </th> <td>   42.76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>    0.3097</td> <td>    0.342</td> <td>    0.905</td> <td> 0.369</td> <td>   -0.375</td> <td>    0.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>               <td>    0.0012</td> <td>    0.000</td> <td>    3.267</td> <td> 0.002</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>              <td> 3.573e-05</td> <td>  8.1e-05</td> <td>    0.441</td> <td> 0.661</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>         <td>   -0.0104</td> <td>    0.031</td> <td>   -0.330</td> <td> 0.743</td> <td>   -0.073</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distance metro</th>    <td>   -0.0173</td> <td>    0.036</td> <td>   -0.477</td> <td> 0.635</td> <td>   -0.090</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distance cemetery</th> <td>   -0.1149</td> <td>    0.010</td> <td>  -11.107</td> <td> 0.000</td> <td>   -0.136</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cats</th>              <td>    0.0370</td> <td>    0.011</td> <td>    3.343</td> <td> 0.001</td> <td>    0.015</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>howls</th>             <td>   -0.0091</td> <td>    0.013</td> <td>   -0.694</td> <td> 0.491</td> <td>   -0.035</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clouds</th>            <td>   -0.1022</td> <td>    0.139</td> <td>   -0.738</td> <td> 0.463</td> <td>   -0.379</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>precipitation</th>     <td>    0.0166</td> <td>    0.076</td> <td>    0.217</td> <td> 0.829</td> <td>   -0.136</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>misery index</th>      <td>    0.0051</td> <td>    0.006</td> <td>    0.802</td> <td> 0.426</td> <td>   -0.008</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ice cream sold</th>    <td>    0.0004</td> <td>    0.001</td> <td>    0.788</td> <td> 0.434</td> <td>   -0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.027</td> <th>  Durbin-Watson:     </th> <td>   1.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.986</td> <th>  Jarque-Bera (JB):  </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.026</td> <th>  Prob(JB):          </th> <td>   0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.764</td> <th>  Cond. No.          </th> <td>2.36e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.36e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                haunted   R-squared:                       0.748\n",
       "Model:                            OLS   Adj. R-squared:                  0.702\n",
       "Method:                 Least Squares   F-statistic:                     16.22\n",
       "Date:                Fri, 01 May 2020   Prob (F-statistic):           3.75e-14\n",
       "Time:                        12:19:42   Log-Likelihood:                 4.2800\n",
       "No. Observations:                  72   AIC:                             15.44\n",
       "Df Residuals:                      60   BIC:                             42.76\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                 0.3097      0.342      0.905      0.369      -0.375       0.994\n",
       "age                   0.0012      0.000      3.267      0.002       0.000       0.002\n",
       "area               3.573e-05    8.1e-05      0.441      0.661      -0.000       0.000\n",
       "bathrooms            -0.0104      0.031     -0.330      0.743      -0.073       0.052\n",
       "distance metro       -0.0173      0.036     -0.477      0.635      -0.090       0.055\n",
       "distance cemetery    -0.1149      0.010    -11.107      0.000      -0.136      -0.094\n",
       "cats                  0.0370      0.011      3.343      0.001       0.015       0.059\n",
       "howls                -0.0091      0.013     -0.694      0.491      -0.035       0.017\n",
       "clouds               -0.1022      0.139     -0.738      0.463      -0.379       0.175\n",
       "precipitation         0.0166      0.076      0.217      0.829      -0.136       0.169\n",
       "misery index          0.0051      0.006      0.802      0.426      -0.008       0.018\n",
       "ice cream sold        0.0004      0.001      0.788      0.434      -0.001       0.002\n",
       "==============================================================================\n",
       "Omnibus:                        0.027   Durbin-Watson:                   1.926\n",
       "Prob(Omnibus):                  0.986   Jarque-Bera (JB):                0.176\n",
       "Skew:                          -0.026   Prob(JB):                        0.916\n",
       "Kurtosis:                       2.764   Cond. No.                     2.36e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.36e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features in 2D array\n",
    "X = df[[\"age\", \"area\", \"bathrooms\", \"distance metro\", \"distance cemetery\", \"cats\", \"howls\", \"clouds\", \"precipitation\", \"misery index\", \"ice cream sold\"]]\n",
    "\n",
    "# Add a constant to the array for the intecept \n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Response data \n",
    "y = df[\"haunted\"]\n",
    "\n",
    "# Fit OLS model \n",
    "model = sm.OLS(y, X).fit() \n",
    "\n",
    "# Print model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "We can see from our model that our F-statistic is 16.22 and our p-value is less than 0.01, indicating we have sufficient evidence to reject the null hypothesis and conlude that at least one of our features is related to the haunted score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Write a function `forward_select(df, resp_str, maxk)` that takes in the DataFrame, the name of the column corresponding to the response, and the maximum number of desired features, and returns a list of feature names corresponding to the `maxk` most important features via forward selection.  At each stage in forward selection you should add the feature whose inclusion in the model would result in the lowest sum of squared errors $(SSE)$. Use your function to determine the best $k=5$ features to include in the model. Clearly indicate which feature was added in each stage. \n",
    "\n",
    "**Note**: The point of this exercise is to see if you can implement **foward_select** yourself.  You may of course use canned routines like statmodels OLS, but you may not call any Python method that explicitly performs forward selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance cemetery  has been added to features\n",
      "Current features:  ['distance cemetery']\n",
      "cats  has been added to features\n",
      "Current features:  ['distance cemetery', 'cats']\n",
      "age  has been added to features\n",
      "Current features:  ['distance cemetery', 'cats', 'age']\n",
      "misery index  has been added to features\n",
      "Current features:  ['distance cemetery', 'cats', 'age', 'misery index']\n",
      "distance metro  has been added to features\n",
      "Current features:  ['distance cemetery', 'cats', 'age', 'misery index', 'distance metro']\n"
     ]
    }
   ],
   "source": [
    "#define feature column names\n",
    "features = [\"age\", \"area\", \"bathrooms\", \"distance metro\", \"distance cemetery\", \"cats\", \"howls\", \"clouds\", \"precipitation\", \"misery index\", \"ice cream sold\"]\n",
    "\n",
    "#define list for features we select in our function\n",
    "f_select = []\n",
    "\n",
    "def forward_select(df, resp_str, maxk):\n",
    "    #define our y value\n",
    "    y = df[resp_str]\n",
    "    \n",
    "    #number of features we have selected\n",
    "    count = 0 \n",
    "    \n",
    "    #loop until we have 5 added features\n",
    "    while count < maxk:\n",
    "        \n",
    "        #SLR for each feature (p) we have in our dataframe\n",
    "        if count == 0: \n",
    "\n",
    "            #calculate the first SLR\n",
    "            X = df[features[0]]\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(y,X).fit()\n",
    "\n",
    "            #set current SSE and lowest SSE to first SLR\n",
    "            SSEcurr = model.ssr\n",
    "            SSElow = model.ssr\n",
    "            SSEname = features[0]\n",
    "\n",
    "            #now calculate SSE for each feature, and replace low SSE\n",
    "            for m in features:\n",
    "                X = df[[m]] #reset X feature\n",
    "                X = sm.add_constant(X)\n",
    "                model = sm.OLS(y,X).fit() #fit model with new X feature\n",
    "                SSEcurr = model.ssr #set current SSE\n",
    "                if SSEcurr < SSElow: #if current is less than low\n",
    "                    SSEname = m \n",
    "                    SSElow = SSEcurr\n",
    "\n",
    "            features.remove(SSEname) #remove name from features\n",
    "            f_select.append(SSEname) #add name to selected features\n",
    "            count += 1 #add 1 to the count\n",
    "            \n",
    "            #print additions to list\n",
    "            print(SSEname, \" has been added to features\")\n",
    "            print(\"Current features: \", f_select)\n",
    "        \n",
    "        #MLR for remaining features (p) we have in our dataframe\n",
    "        if count != 0: \n",
    "\n",
    "            #loop through features\n",
    "            for m in features:\n",
    "                f_select.append(m) #add to list\n",
    "                X = df[f_select] #reset x to be previous added + m\n",
    "                f_select.remove(m) #remove m so we don't lose track of added features\n",
    "                X = sm.add_constant(X)\n",
    "                model = sm.OLS(y,X).fit() #fit model with new X feature\n",
    "                SSEcurr = model.ssr #set current SSE\n",
    "\n",
    "                if SSEcurr < SSElow:\n",
    "                    SSEname = m\n",
    "                    SSElow = SSEcurr\n",
    "\n",
    "            features.remove(SSEname) #remove name from features\n",
    "            f_select.append(SSEname) #add name to selected features\n",
    "            count += 1 #add 1 to the count\n",
    "            \n",
    "            print(SSEname, \" has been added to features\")\n",
    "            print(\"Current features: \", f_select)\n",
    "                      \n",
    "forward_select(df, 'haunted', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write down the multiple linear regression model, including estimated parameters, obtained by your forward selection process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>haunted</td>     <th>  R-squared:         </th> <td>   0.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   38.12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 01 May 2020</td> <th>  Prob (F-statistic):</th> <td>3.40e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:19:51</td>     <th>  Log-Likelihood:    </th> <td>  3.4850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    72</td>      <th>  AIC:               </th> <td>   5.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    66</td>      <th>  BIC:               </th> <td>   18.69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>    0.3848</td> <td>    0.240</td> <td>    1.607</td> <td> 0.113</td> <td>   -0.093</td> <td>    0.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distance cemetery</th> <td>   -0.1114</td> <td>    0.009</td> <td>  -12.215</td> <td> 0.000</td> <td>   -0.130</td> <td>   -0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cats</th>              <td>    0.0348</td> <td>    0.010</td> <td>    3.398</td> <td> 0.001</td> <td>    0.014</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>               <td>    0.0012</td> <td>    0.000</td> <td>    3.529</td> <td> 0.001</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>misery index</th>      <td>    0.0058</td> <td>    0.006</td> <td>    0.962</td> <td> 0.339</td> <td>   -0.006</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distance metro</th>    <td>   -0.0269</td> <td>    0.032</td> <td>   -0.830</td> <td> 0.409</td> <td>   -0.092</td> <td>    0.038</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.218</td> <th>  Durbin-Watson:     </th> <td>   1.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.897</td> <th>  Jarque-Bera (JB):  </th> <td>   0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.034</td> <th>  Prob(JB):          </th> <td>   0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.636</td> <th>  Cond. No.          </th> <td>1.38e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.38e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                haunted   R-squared:                       0.743\n",
       "Model:                            OLS   Adj. R-squared:                  0.723\n",
       "Method:                 Least Squares   F-statistic:                     38.12\n",
       "Date:                Fri, 01 May 2020   Prob (F-statistic):           3.40e-18\n",
       "Time:                        12:19:51   Log-Likelihood:                 3.4850\n",
       "No. Observations:                  72   AIC:                             5.030\n",
       "Df Residuals:                      66   BIC:                             18.69\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                 0.3848      0.240      1.607      0.113      -0.093       0.863\n",
       "distance cemetery    -0.1114      0.009    -12.215      0.000      -0.130      -0.093\n",
       "cats                  0.0348      0.010      3.398      0.001       0.014       0.055\n",
       "age                   0.0012      0.000      3.529      0.001       0.001       0.002\n",
       "misery index          0.0058      0.006      0.962      0.339      -0.006       0.018\n",
       "distance metro       -0.0269      0.032     -0.830      0.409      -0.092       0.038\n",
       "==============================================================================\n",
       "Omnibus:                        0.218   Durbin-Watson:                   1.957\n",
       "Prob(Omnibus):                  0.897   Jarque-Bera (JB):                0.410\n",
       "Skew:                          -0.034   Prob(JB):                        0.815\n",
       "Kurtosis:                       2.636   Cond. No.                     1.38e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.38e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['haunted']\n",
    "X = df[f_select]\n",
    "X = sm.add_constant(X)\n",
    "model_reduced = sm.OLS(y,X).fit() #fit model with new X feature\n",
    "\n",
    "model_reduced.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLR Model Including Estimated Parameters:**\n",
    "\n",
    "Our estimated model will be in the form $\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{dc}x_{dc} + \\hat{\\beta}_{cats}x_{cats} +\\hat{\\beta}_{age}x_{age} +\\hat{\\beta}_{mi}x_{mi} +\\hat{\\beta}_{dm}x_{dm}$ where dc is distance cemetery, mi is misery index, and dm distance metro.\n",
    "\n",
    "Using our estimated parameters from our model, we have:\n",
    "\n",
    "$\\hat{y} = 0.3848 + -0.1114x_{dc} + 0.0348x_{cats} + 0.0012x_{age} +0.0058x_{mi} + -0.0269x_{dm}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Perform the appropriate statistical test at the $\\alpha = 0.05$ significance level to determine whether there is a statistically significant difference between the full model with all features and the reduced model obtained by forward selection in **Part D**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Setup:**\n",
    "\n",
    "We will use a partial F-test to determine whether there is a statistically significant difference between the full model with all features and the reduced model.  Our hypotheses will be as follows:\n",
    "\n",
    "$H_{0}:  \\beta_{area} = \\beta_{bathrooms} = \\beta_{howls} = \\beta_{clouds} = \\beta_{precip} = \\beta_{icecreamsold} = 0$ (None of these features are related to to the haunted score)\n",
    "\n",
    "$H_{1}:$  At least one $\\beta$ above $\\neq 0$\n",
    "\n",
    "$SSE_{full}$ = variation unexplained by the full model\n",
    "\n",
    "$SSE_{reduced}$ = variation unexplained by the reduced model\n",
    "\n",
    "Our appropriate F-statistic should depend on the difference between SSE's for each model.  In our case, variables we need to calculate the F-statistic are as follows:\n",
    "\n",
    "p = 11\n",
    "\n",
    "k = 5\n",
    "\n",
    "n = 72\n",
    "\n",
    "We will also get the SSE's for each model using statsmodels.api built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2232889261219416\n",
      "2.627369592102272\n"
     ]
    }
   ],
   "source": [
    "p = 11\n",
    "k = 5\n",
    "n = 72\n",
    "\n",
    "# Get SSE's from each model\n",
    "SSE_full = model.ssr\n",
    "SSE_reduced = model_reduced.ssr\n",
    "\n",
    "#calculate F-statistic\n",
    "f_stat = ((SSE_reduced - SSE_full)/(p-k)) / (SSE_full/(n-p-1))\n",
    "\n",
    "print(f_stat)\n",
    "\n",
    "f_alpha = stats.f.ppf(1-(0.05/2),(p-k),(n-p-1)) #95% confidence interval\n",
    "print(f_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "We can see from our results that $F_{stat} < F_{alpha}$, therefore we fail to reject the null hypothesis and conlude that our reduced model is statistically better than the full model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Based on your conclusions in **Part E**, use the _better_ of the two models to predict the haunted house hazard when the following features are observed: \n",
    "\n",
    "- $\\texttt{age}$: 150 years\n",
    "- $\\texttt{area}$: 2200 square feet\n",
    "- $\\texttt{bathrooms}$: 3 bathrooms\n",
    "- $\\texttt{distance metro}$: 25 miles\n",
    "- $\\texttt{distance cemetery}$: 5 miles\n",
    "- $\\texttt{cats}$: 20 cats\n",
    "- $\\texttt{howls}$: 5 wolf howls/night\n",
    "- $\\texttt{clouds}$: 0.65 cloud cover\n",
    "- $\\texttt{precipitation}$: 0 inches\n",
    "- $\\texttt{misery index}$: 10\n",
    "We- $\\texttt{ice cream sold}$: 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following formula to predict our value based on the observed features above:\n",
    "\n",
    "$\\hat{y} = 0.3848 + -0.1114x_{dc} + 0.0348x_{cats} + 0.0012x_{age} +0.0058x_{mi} + -0.0269x_{dm}$\n",
    "\n",
    "As computed below, we can see the resulting haunted score is 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08930000000000005\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "\n",
    "yhat = 0.3848 - 0.1114*5 + 0.0348*20 + 0.0012*150 + 0.0058*10 - 0.0269*25\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G:** Governor Polis dabbles a bit in the art of data science, as well as the science of data art. He tells you that the response (`haunted` score) that you and Amy predicted is actually the natural logarithm of the _odds_ that a house with the given features is haunted, where if $p$ is the probability that a house is haunted, then the odds are given by $$\\text{odds} = \\dfrac{p}{1-p}$$\n",
    "\n",
    "So using Govenor Polis's information and combining it with our knowledge of logistic regression, the probability of a house being haunted is:\n",
    "\n",
    "$$p(\\text{haunted} \\mid x_1, x_2, \\ldots) = \\dfrac{1}{1+\\texttt{exp}[-(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)]}$$\n",
    "\n",
    "Perform this computation, then use a decision threshold of 0.5 to classify the house from **Part F** as haunted or not haunted. No new models should be fit here; use the same model that you used in Part F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [50 points] Problem 2: Amazon Forest Fires\n",
    "A non-profit trying to protect the amazon rain forest has recruited you to join their data science corps. For your first task, they've given you a dataset with the number of reported forest fires in each state in the Amazon region of Brazil during each month between 1998 and 2017. The Brazilian government has 500 extra wildland firefighters and they have asked your non-profit to determine which state or states they should allocate these firefighters to during each month of the year. To do this, they want you to calculate an 80% confidence interval for the mean and median number of fires that occur during each month for each state, and use those statistics to determine where the firefighters should be assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: Loading The CSV\n",
    "Read the csv located in `amazon.csv` into a pandas data frame. Brazil and many other countries use the period (.) symbol as a thousands separator and a comma (,) as the decimal separator. Ex. One Thousand And $\\frac{75}{100}$ would be represented as $1.000,75$ instead of the familiar english notation $1,000.75$. When you read it in, you'll need to use a period(.) as the thousands separator and a comma(,) as the decimal separator. Because the comma is already in use as the decimal separator, this file uses a different character to separate columns in the data. Open up the file in a text editor and figure out what character was used. Then find the correct arguments to `pd.read_csv` to read in this file properly. Look up the docs if you're unsure what the arguments you'll need are. Print out the `.info` summary of the dataframe after you've read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>month</th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Janeiro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1998-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1999</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Janeiro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Janeiro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Janeiro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2001-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2002</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Janeiro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6449</td>\n",
       "      <td>2012</td>\n",
       "      <td>Tocantins</td>\n",
       "      <td>Dezembro</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tocantins</td>\n",
       "      <td>Dezembro</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6451</td>\n",
       "      <td>2014</td>\n",
       "      <td>Tocantins</td>\n",
       "      <td>Dezembro</td>\n",
       "      <td>223.0</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6452</td>\n",
       "      <td>2015</td>\n",
       "      <td>Tocantins</td>\n",
       "      <td>Dezembro</td>\n",
       "      <td>373.0</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6453</td>\n",
       "      <td>2016</td>\n",
       "      <td>Tocantins</td>\n",
       "      <td>Dezembro</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6454 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year      state     month  number        date\n",
       "0     1998       Acre   Janeiro     0.0  1998-01-01\n",
       "1     1999       Acre   Janeiro     0.0  1999-01-01\n",
       "2     2000       Acre   Janeiro     0.0  2000-01-01\n",
       "3     2001       Acre   Janeiro     0.0  2001-01-01\n",
       "4     2002       Acre   Janeiro     0.0  2002-01-01\n",
       "...    ...        ...       ...     ...         ...\n",
       "6449  2012  Tocantins  Dezembro   128.0  2012-01-01\n",
       "6450  2013  Tocantins  Dezembro    85.0  2013-01-01\n",
       "6451  2014  Tocantins  Dezembro   223.0  2014-01-01\n",
       "6452  2015  Tocantins  Dezembro   373.0  2015-01-01\n",
       "6453  2016  Tocantins  Dezembro   119.0  2016-01-01\n",
       "\n",
       "[6454 rows x 5 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"amazon.csv\"\n",
    "amazon_fires = pd.read_csv(file_path, sep=';', thousands='.', decimal=',')\n",
    "amazon_fires\n",
    "\n",
    "#https://stackoverflow.com/questions/22137723/convert-number-strings-with-commas-in-pandas-dataframe-to-float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Data Cleaning\n",
    "\n",
    "This dataset isn't paticularly useful in it's current state, so we'll need to clean it up a bit. Some data scientists say that most of their job is to wrangle data, so this will give you a taste of cleaning a real world data set. Perform the following tasks. \n",
    "1. Drop the 'date' column. The only information this column holds is the year, which we already have in another column. Use the `.info` summary provided to check your work.\n",
    "2. Drop any rows with null values in any of the remaining columns. Use the provided code to print the number of rows remaining after this step.\n",
    "3. Print all the unique values of the 'month' column. You'll notice that one is encoded with a different character encoding then the format that pandas is using.\n",
    "4. Convert the Portugese month names to English month names. If you'd like to use them, we've included the 'month_name' and the 'different_encoding' modules of the python calendar library. There are many ways to accomplish this task, and these modules are not required, but may make things easier. As part of this step, you should make sure that the Portugese month with the encoding problem is translated to the correct english month. Use the `.unique` method provided for you to check your work. \n",
    "5. Check the number column for any values that seem impossible. If you find any values you think are impossible, drop them. As a guidline, we would never expect a single state to have more than 50,000 reported forest fires in a single month. Also keep in mind that we are tracking forest fires here. Do negative or fractional forest fires really make sense? You should check for any obivously impossible conditions that you think might occur, and drop rows accordingly. Use the provided code to print the number of rows remaining after this step.\n",
    "6. Since you're new on the job, some of your co-workers may have played a prank on you... Print out all the unique values of the 'year' column and drop any rows with values that don't make sense. Use the provided code to print the number of rows remaining after this step.\n",
    "7. For every state in the data, print the number of rows the state has associated with it. A number of states have far more observations than the others. Each state should have roughly 240 observations (20 years multiplied by 12 months/year minus any bad data). Drop all the observations for any states that have more than 240 rows associated with them. For two points of extra credit, figure out why these states have way more rows associated with them than they should. If you choose to do the extra credit, put your answer in the markdown cell below. \n",
    "8. To give you an idea of whether your answer is correct, we've provided a unit test below the last cell. It should pass. If it doesn't, go back and figure out which step has gone awry.\n",
    "\n",
    "We've given you a code cell for each task to make organizing the grading a bit easier. Please perform step 1 in the first code cell and so on.\n",
    "\n",
    "**NOTE:** Since some of these tasks are not totally trivial, you may use any resources other than your classmates on this part of this problem. This means you may consult google, stack overflow, the python/pandas documentation, some random book on pandas you might have, etc... But you may not consult your classmates for help. We will also be more helpful on this problem in office hours and in response to your *private* piazza messages.  ***CITE ALL RESOURCES USED IN A CODE COMMENT. A URL OR A BOOK TITLE IS SUFFICIENT. ANY CODE OBIVOUSLY COPIED FROM OUTSIDE SOURCES WITH OUT A CITATION WILL EARN YOU NO CREDIT ON THIS PROBLEM.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6454 entries, 0 to 6453\n",
      "Data columns (total 4 columns):\n",
      "year      6453 non-null object\n",
      "state     6452 non-null object\n",
      "month     6454 non-null object\n",
      "number    6448 non-null float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 201.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 1 here:\n",
    "amazon_fires = amazon_fires.drop(columns = ['date'])\n",
    "amazon_fires.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6446\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 2 here:\n",
    "#drop any rows with missing data\n",
    "amazon_fires = amazon_fires.dropna(how='any')\n",
    "print(len(amazon_fires))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Janeiro' 'Fevereiro' 'Mar�o' 'Abril' 'Maio' 'Junho' 'Julho' 'Agosto'\n",
      " 'Setembro' 'Outubro' 'Novembro' 'Dezembro']\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 3 here:\n",
    "print(amazon_fires['month'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['January' 'February' 'March' 'April' 'May' 'June' 'July' 'August'\n",
      " 'September' 'October' 'November' 'December']\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 4 here:\n",
    "d = {u'Janeiro':'January', u'Fevereiro':'February', u'Mar�o':'March', u'Abril':'April', u'Maio':'May', u'Junho':'June', u'Julho':'July', u'Agosto':'August', u'Setembro':'September', u'Outubro':'October', u'Novembro':'November', u'Dezembro':'December'}\n",
    "\n",
    "amazon_fires['month'] = amazon_fires['month'].replace(d, regex=True)\n",
    "\n",
    "print(amazon_fires['month'].unique())\n",
    "\n",
    "#reference stackoverflow\n",
    "#https://stackoverflow.com/questions/37114683/pandas-replace-months-names-within-dataframe-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6441\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 5\n",
    "\n",
    "#drop any with negative fires or fires between 0 and 1 (doesn't make sense)\n",
    "amazon_fires = amazon_fires.loc[(amazon_fires.number == 0.0) | (amazon_fires.number >= 1.0)] \n",
    "\n",
    "#drop any with more than 50,000 fires (per directions)\n",
    "amazon_fires = amazon_fires.loc[amazon_fires.number <= 50000]\n",
    "\n",
    "#multi-step to drop fractional values\n",
    "\n",
    "#first add a column that rounds number down and converts to type int\n",
    "amazon_fires['rounded'] = amazon_fires['number'].apply(np.floor).astype(int)\n",
    "\n",
    "#add another column that divides number by rounded\n",
    "amazon_fires['div'] = amazon_fires.number / amazon_fires.rounded\n",
    "\n",
    "#replace NaN with 1.0 (anything / 0 will result in NaN and we want to keep 0 values in df)\n",
    "amazon_fires['div'] = amazon_fires['div'].replace(np.nan,1.0)\n",
    "\n",
    "#keep non-decimal values\n",
    "amazon_fires = amazon_fires.loc[amazon_fires['div'] == 1.0] \n",
    "\n",
    "print(len(amazon_fires))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1998' '1999' '2000' '2001' '2002' '2003' '2004' '2005' '2006' '2007'\n",
      " '2008' '2009' '2010' '2011' '2012' '2013' '2014' '2015' '2016' '2017'\n",
      " '1000bc' '-40' '10bc' \"our new data scientist won't notice this\"]\n",
      "['1998' '1999' '2000' '2001' '2002' '2003' '2004' '2005' '2006' '2007'\n",
      " '2008' '2009' '2010' '2011' '2012' '2013' '2014' '2015' '2016' '2017']\n",
      "6437\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 6\n",
    "print(amazon_fires['year'].unique())\n",
    "\n",
    "#drop non-numeric values\n",
    "amazon_fires = amazon_fires[amazon_fires.year.apply(lambda x: x.isnumeric())]\n",
    "print(amazon_fires['year'].unique())\n",
    "\n",
    "print(len(amazon_fires))\n",
    "\n",
    "#https://stackoverflow.com/questions/33961028/remove-non-numeric-rows-in-one-column-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acre 239\n",
      "Alagoas 240\n",
      "Amapa 239\n",
      "Amazonas 239\n",
      "Bahia 239\n",
      "Ceara 239\n",
      "Distrito Federal 239\n",
      "Espirito Santo 238\n",
      "Goias 238\n",
      "Maranhao 239\n",
      "Mato Grosso 476\n",
      "Minas Gerais 237\n",
      "Para 235\n",
      "Paraiba 474\n",
      "Pernambuco 239\n",
      "Piau 239\n",
      "Rio 715\n",
      "Rondonia 239\n",
      "Roraima 239\n",
      "Santa Catarina 238\n",
      "Sao Paulo 239\n",
      "Sergipe 239\n",
      "Tocantins 239\n"
     ]
    }
   ],
   "source": [
    "#Code for data cleaning task 7\n",
    "\n",
    "#get all unique states\n",
    "states = amazon_fires['state'].unique()\n",
    "\n",
    "for s in states:\n",
    "    length = len(amazon_fires[(amazon_fires['state'] == s)]) ## get the lenght of rows associated with state \"s\"\n",
    "    print(s, length) # print the states and number of rows\n",
    "    if length > 240: # drop if greater than 240\n",
    "        amazon_fires = amazon_fires.loc[amazon_fires['state'] != s] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given Test. This Cell Should Not Throw an Exception!\n",
    "assert \\\n",
    "    len(amazon_fires['state'].unique()) == 20 and \\\n",
    "    list(amazon_fires['month'].unique()) == \\\n",
    "        ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "             'July', 'August','September', 'October', 'November',\n",
    "             'December'] and \\\n",
    "    len(amazon_fires) == 4772, 'somethings wrong in problem 1.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C: Medians and Means!\n",
    "In this part of the problem, we'll calculate an 80% confidence interval for both the mean and median number of wildfires each state has during each month of the year. \n",
    "\n",
    "For the mean you should use the appropriate confidence interval with the correct distribution. Remember to check how many observations we have. Use the sample standard deviation. \n",
    "\n",
    "For the median, we'll have to bootstrap it because the median is not known to be normally distributed. You should bootstrap 1000 samples of the same length as the original sample for each month for each state. Calculate the median for each bootstrapped sample. Then take the middle 80% of the bootstrapped medians as your confidnce interval. This is called a bootstrapped percentile median. There are a few more complex and slightly more rigourous ways to estimate the median from bootstrapped samples, but this will serve for our purposes.\n",
    "\n",
    "You're given a dictionary of dictionaries to store your confidence intervals for the medians and means in. \n",
    "\n",
    "Take a look at the dictionary structure below. \n",
    "\n",
    "The idea here is that for every month, for every state, you will fill in the `mean_CI` with a length two list that contains the low and high end of the confidence interval for the true mean number of fires for that state in that month. \n",
    "\n",
    "Similiarly, for every month, for every state, you will fill in the `median_CI` with a length two list that contains the low and high end of the confidence interval for the true median number of fires for that state in that month.\n",
    "\n",
    "For example:\n",
    "\n",
    "When you're done `months['January']['Acre']['mean_CI']` should be a list with the low and high bounds for the confidence interval of the true mean number of wildfires in the state of Acre in January. So `months['January']['Acre']['mean_CI'][0]` should be the low end of the CI for the mean, and `months['January']['Acre']['mean_CI'][1]` should be the high end of the CI for the mean.\n",
    "\n",
    "`months['January']['Acre']['median_CI']` should hold the confidence interval for the true median number of wildfires in the state of Acre in January. So `months['January']['Acre']['median_CI'][0]` should be the low end of the CI for the median, and `months['January']['Acre']['median_CI'][1]` should be the high end of the CI for the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN CODE DO NOT CHANGE THIS!!!\n",
    "#YOU SHOULD BE WRITING CODE IN THE NEXT CELL(s) THAT FILLS IN THE 'months' DICTIONARY.\n",
    "\n",
    "#If you're curious what copy and deep copy do and why we used them here see an explanation \n",
    "#here: https://thispointer.com/python-how-to-copy-a-dictionary-shallow-copy-vs-deep-copy/\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "mean_median_dict ={\n",
    "    'mean_CI' : None,\n",
    "    'median_CI': None\n",
    "}\n",
    "\n",
    "CI_median_num_fires = {\n",
    "    'Acre': dict(mean_median_dict),\n",
    "    'Alagoas':dict( mean_median_dict),\n",
    "    'Amapa':dict( mean_median_dict),\n",
    "    'Amazonas':dict( mean_median_dict),\n",
    "    'Bahia':dict( mean_median_dict),\n",
    "    'Ceara':dict( mean_median_dict),\n",
    "    'Distrito Federal':dict( mean_median_dict),\n",
    "    'Espirito Santo':dict( mean_median_dict),\n",
    "    'Goias':dict( mean_median_dict),\n",
    "    'Maranhao':dict( mean_median_dict),\n",
    "    'Minas Gerais':dict( mean_median_dict),\n",
    "    'Para':dict( mean_median_dict),\n",
    "    'Pernambuco':dict( mean_median_dict),\n",
    "    'Piau':dict( mean_median_dict),\n",
    "    'Rondonia':dict( mean_median_dict),\n",
    "    'Roraima':dict( mean_median_dict),\n",
    "    'Santa Catarina':dict( mean_median_dict),\n",
    "    'Sao Paulo':dict( mean_median_dict),\n",
    "    'Sergipe':dict( mean_median_dict),\n",
    "    'Tocantins':dict( mean_median_dict)  \n",
    "}\n",
    "\n",
    "months = {\n",
    "    'January': deepcopy(CI_median_num_fires),\n",
    "    'February': deepcopy(CI_median_num_fires),\n",
    "    'March': deepcopy(CI_median_num_fires), \n",
    "    'April': deepcopy(CI_median_num_fires), \n",
    "    'May': deepcopy(CI_median_num_fires),\n",
    "    'June': deepcopy(CI_median_num_fires),\n",
    "    'July': deepcopy(CI_median_num_fires),\n",
    "    'August': deepcopy(CI_median_num_fires), \n",
    "    'September': deepcopy(CI_median_num_fires), \n",
    "    'October': deepcopy(CI_median_num_fires),\n",
    "    'November': deepcopy(CI_median_num_fires),\n",
    "    'December': deepcopy(CI_median_num_fires)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CI for Mean:**\n",
    "\n",
    "Each month has at max 20 observations per state, which means we have n < 30 for our sample population size for any given state, in any given month.  Thus, we must use the t-distribution in calculating our mean CI's for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATION OF MEAN CI's\n",
    "\n",
    "#get all unique states and months in an array, respectively\n",
    "states = amazon_fires['state'].unique()\n",
    "mnths = amazon_fires['month'].unique()\n",
    "\n",
    "#loop through each state\n",
    "for s in states:\n",
    "    tempDF = amazon_fires.loc[amazon_fires['state'] == s] #get a temp dataframe only \n",
    "    \n",
    "    #loop through each month\n",
    "    for m in mnths:\n",
    "        monthly_data = tempDF.loc[tempDF['month'] == m] #get only data related to month\n",
    "        n = len(monthly_data[(monthly_data['month'] == m)]) #get the length of the dataframe\n",
    "        xbar = monthly_data['number'].mean(axis=0) #calculate xbar\n",
    "        stdv = monthly_data['number'].std(axis=0) #calculate sample standard deviation\n",
    "        t_alpha = stats.t.ppf(1-(0.2/2), n-1) #80% confidence interval\n",
    "        SE = stdv/np.sqrt(n)\n",
    "        CImin = xbar - t_alpha*SE\n",
    "        CImax = xbar + t_alpha*SE\n",
    "        months[m][s]['mean_CI'] = [CImin,CImax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATION OF MEDIAN CI's\n",
    "\n",
    "samples = 1000 #number of samples to take\n",
    "\n",
    "for s in states:\n",
    "    tempDF = amazon_fires.loc[amazon_fires['state'] == s] #get a temp dataframe only \n",
    "    \n",
    "    #loop through each month\n",
    "    for m in mnths:\n",
    "        monthly_data = tempDF.loc[tempDF['month'] == m] #get only data related to month\n",
    "        n = len(monthly_data[(monthly_data['month'] == m)]) #get the length of the dataframe\n",
    "        monthly_data.reset_index(inplace = True) #reset index\n",
    "        count = 0 #reset count\n",
    "        medians = [] #reset medians array\n",
    "        \n",
    "        while count < samples:#take 1000 resamples\n",
    "            draws = []\n",
    "            draws = random.choices(monthly_data.number, k=n) #draw n choices\n",
    "            med = np.median(draws) #get median of sample\n",
    "            medians.append(med) #append to list\n",
    "            count += 1 #increment count\n",
    "            \n",
    "        months[m][s]['median_CI'] = np.percentile(medians, [10,90]) #set median CI's to percentile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'April': {'Acre': {'mean_CI': [0.7631702958582414, 3.336829704141758],\n",
      "                    'median_CI': array([0., 1.])},\n",
      "           'Alagoas': {'mean_CI': [7.979189204540319, 16.52081079545968],\n",
      "                       'median_CI': array([ 4.5, 16. ])},\n",
      "           'Amapa': {'mean_CI': [0.3412383897376437, 0.9587616102623564],\n",
      "                     'median_CI': array([0., 0.])},\n",
      "           'Amazonas': {'mean_CI': [6.9737040719879255, 12.226295928012075],\n",
      "                        'median_CI': array([ 3.5, 12. ])},\n",
      "           'Bahia': {'mean_CI': [93.8962665362685, 157.1037334637315],\n",
      "                     'median_CI': array([ 76. , 121.5])},\n",
      "           'Ceara': {'mean_CI': [2.695004965166694, 4.904995034833306],\n",
      "                     'median_CI': array([2., 4.])},\n",
      "           'Distrito Federal': {'mean_CI': [0.6266818228169977,\n",
      "                                            1.8733181771830023],\n",
      "                                'median_CI': array([0., 1.])},\n",
      "           'Espirito Santo': {'mean_CI': [7.3942600245658525,\n",
      "                                          19.20573997543415],\n",
      "                              'median_CI': array([2.5, 8. ])},\n",
      "           'Goias': {'mean_CI': [46.4030589940258, 72.49694100597421],\n",
      "                     'median_CI': array([35.35, 71.  ])},\n",
      "           'Maranhao': {'mean_CI': [36.39274822543095, 62.10725177456905],\n",
      "                        'median_CI': array([21., 62.])},\n",
      "           'Minas Gerais': {'mean_CI': [69.8507940933388, 104.24920590666119],\n",
      "                            'median_CI': array([ 57.95, 104.  ])},\n",
      "           'Para': {'mean_CI': [12.594826892563928, 22.705173107436067],\n",
      "                    'median_CI': array([ 9.5, 20. ])},\n",
      "           'Pernambuco': {'mean_CI': [5.168123673457631, 12.031876326542369],\n",
      "                          'median_CI': array([2., 7.])},\n",
      "           'Piau': {'mean_CI': [19.061044614180673, 38.03895538581933],\n",
      "                    'median_CI': array([ 9.5, 29.5])},\n",
      "           'Rondonia': {'mean_CI': [6.288980064343675, 14.111019935656323],\n",
      "                        'median_CI': array([ 3. , 10.5])},\n",
      "           'Roraima': {'mean_CI': [83.94268625712523, 147.05731374287478],\n",
      "                       'median_CI': array([ 49.15, 138.5 ])},\n",
      "           'Santa Catarina': {'mean_CI': [20.41782546524641,\n",
      "                                          37.282174534753594],\n",
      "                              'median_CI': array([16., 33.])},\n",
      "           'Sao Paulo': {'mean_CI': [74.35052638450803, 120.54947361549198],\n",
      "                         'median_CI': array([63., 93.])},\n",
      "           'Sergipe': {'mean_CI': [8.50960400315784, 16.59039599684216],\n",
      "                       'median_CI': array([ 7. , 12.5])},\n",
      "           'Tocantins': {'mean_CI': [64.25183572008902, 135.648164279911],\n",
      "                         'median_CI': array([34. , 88.3])}},\n",
      " 'August': {'Acre': {'mean_CI': [732.551362692288, 1308.748637307712],\n",
      "                     'median_CI': array([562. , 969.5])},\n",
      "            'Alagoas': {'mean_CI': [1.5692280861125414, 2.4307719138874586],\n",
      "                        'median_CI': array([1., 3.])},\n",
      "            'Amapa': {'mean_CI': [10.1870292911088, 17.2129707088912],\n",
      "                      'median_CI': array([ 7., 16.])},\n",
      "            'Amazonas': {'mean_CI': [1549.818949495862, 2533.881050504138],\n",
      "                         'median_CI': array([1239.85, 2241.  ])},\n",
      "            'Bahia': {'mean_CI': [951.9226652648525, 1429.1773347351475],\n",
      "                      'median_CI': array([ 776.5, 1018. ])},\n",
      "            'Ceara': {'mean_CI': [96.53564912286369, 143.96435087713633],\n",
      "                      'median_CI': array([ 72.7, 127. ])},\n",
      "            'Distrito Federal': {'mean_CI': [46.384514887361654,\n",
      "                                             73.21548511263833],\n",
      "                                 'median_CI': array([35.5, 54. ])},\n",
      "            'Espirito Santo': {'mean_CI': [35.33949188622248,\n",
      "                                           53.06050811377752],\n",
      "                               'median_CI': array([29.5, 47. ])},\n",
      "            'Goias': {'mean_CI': [1002.4891772607183, 1411.9318753708603],\n",
      "                      'median_CI': array([ 902., 1255.])},\n",
      "            'Maranhao': {'mean_CI': [2533.256803946942, 4111.843196053058],\n",
      "                         'median_CI': array([1963., 3602.])},\n",
      "            'Minas Gerais': {'mean_CI': [1261.9765143962732,\n",
      "                                         1745.6234856037267],\n",
      "                             'median_CI': array([ 959., 1642.])},\n",
      "            'Para': {'mean_CI': [7073.712359985908, 9753.287640014092],\n",
      "                     'median_CI': array([6255., 8555.])},\n",
      "            'Pernambuco': {'mean_CI': [18.013008453040193, 24.986991546959807],\n",
      "                           'median_CI': array([17., 27.])},\n",
      "            'Piau': {'mean_CI': [1249.7365220975278, 1905.463477902472],\n",
      "                     'median_CI': array([ 927., 1765.])},\n",
      "            'Rondonia': {'mean_CI': [2705.34909139971, 3864.55090860029],\n",
      "                         'median_CI': array([2203., 3675.])},\n",
      "            'Roraima': {'mean_CI': [6.974172955075677, 13.725827044924323],\n",
      "                        'median_CI': array([ 4., 10.])},\n",
      "            'Santa Catarina': {'mean_CI': [345.9715148403982,\n",
      "                                           565.4284851596018],\n",
      "                               'median_CI': array([313.5, 582. ])},\n",
      "            'Sao Paulo': {'mean_CI': [654.0535256479883, 890.1464743520118],\n",
      "                          'median_CI': array([576.5, 856. ])},\n",
      "            'Sergipe': {'mean_CI': [0.035976868873909995, 0.86402313112609],\n",
      "                        'median_CI': array([0., 0.])},\n",
      "            'Tocantins': {'mean_CI': [2162.1051262058404, 3140.49487379416],\n",
      "                          'median_CI': array([1793., 2917.])}},\n",
      " 'December': {'Acre': {'mean_CI': [1.590449896692029, 3.356918524360603],\n",
      "                       'median_CI': array([0., 2.])},\n",
      "              'Alagoas': {'mean_CI': [38.619887591043124, 62.85379661948319],\n",
      "                          'median_CI': array([32., 50.])},\n",
      "              'Amapa': {'mean_CI': [168.81582490720234, 305.2894382506924],\n",
      "                        'median_CI': array([ 74., 354.])},\n",
      "              'Amazonas': {'mean_CI': [75.45000458681199, 158.65525857108275],\n",
      "                           'median_CI': array([ 44., 123.])},\n",
      "              'Bahia': {'mean_CI': [208.56853945368238, 419.4314605463176],\n",
      "                        'median_CI': array([144., 259.])},\n",
      "              'Ceara': {'mean_CI': [947.2500163285458, 1412.43419419777],\n",
      "                        'median_CI': array([ 819., 1360.])},\n",
      "              'Distrito Federal': {'mean_CI': [0.09164321209651968,\n",
      "                                               0.43467257737716447],\n",
      "                                   'median_CI': array([0., 0.])},\n",
      "              'Espirito Santo': {'mean_CI': [8.04127548086014,\n",
      "                                             17.116619255981966],\n",
      "                                 'median_CI': array([ 5., 12.])},\n",
      "              'Goias': {'mean_CI': [38.06882475430824, 67.93117524569176],\n",
      "                        'median_CI': array([18., 64.])},\n",
      "              'Maranhao': {'mean_CI': [1576.4725067225977, 2053.5274932774023],\n",
      "                           'median_CI': array([1399., 1803.])},\n",
      "              'Minas Gerais': {'mean_CI': [79.30836867294163,\n",
      "                                           131.35829799372505],\n",
      "                               'median_CI': array([ 44., 108.])},\n",
      "              'Para': {'mean_CI': [2105.311196675936, 3222.022136657397],\n",
      "                       'median_CI': array([1596.5, 3118. ])},\n",
      "              'Pernambuco': {'mean_CI': [142.96849935599437,\n",
      "                                         208.61044801242667],\n",
      "                             'median_CI': array([117., 203.])},\n",
      "              'Piau': {'mean_CI': [315.5100735531379, 424.4899264468621],\n",
      "                       'median_CI': array([334., 374.])},\n",
      "              'Rondonia': {'mean_CI': [55.763728407322205, 109.81521896109884],\n",
      "                           'median_CI': array([  9., 143.])},\n",
      "              'Roraima': {'mean_CI': [95.41151168626455, 154.79901462952495],\n",
      "                          'median_CI': array([ 76., 157.])},\n",
      "              'Santa Catarina': {'mean_CI': [31.874984121300304,\n",
      "                                             51.17764745764707],\n",
      "                                 'median_CI': array([23., 45.])},\n",
      "              'Sao Paulo': {'mean_CI': [44.0084850854865, 62.20204123030298],\n",
      "                            'median_CI': array([45., 57.])},\n",
      "              'Sergipe': {'mean_CI': [19.849169343560167, 35.624514866966145],\n",
      "                          'median_CI': array([14., 33.])},\n",
      "              'Tocantins': {'mean_CI': [54.06502509882489, 108.98760648012248],\n",
      "                            'median_CI': array([21., 96.])}},\n",
      " 'February': {'Acre': {'mean_CI': [0.46382852023648685, 1.436171479763513],\n",
      "                       'median_CI': array([0. , 0.5])},\n",
      "              'Alagoas': {'mean_CI': [20.225921829266387, 30.07407817073361],\n",
      "                          'median_CI': array([17.5, 29.5])},\n",
      "              'Amapa': {'mean_CI': [0.6028479188961431, 1.397152081103857],\n",
      "                        'median_CI': array([0., 1.])},\n",
      "              'Amazonas': {'mean_CI': [33.89548912769326, 70.80451087230674],\n",
      "                           'median_CI': array([23. , 55.5])},\n",
      "              'Bahia': {'mean_CI': [137.16289301430163, 191.33710698569837],\n",
      "                        'median_CI': array([122., 200.])},\n",
      "              'Ceara': {'mean_CI': [13.795479747822013, 26.40452025217799],\n",
      "                        'median_CI': array([ 9. , 20.5])},\n",
      "              'Distrito Federal': {'mean_CI': [0.3412383897376437,\n",
      "                                               0.9587616102623564],\n",
      "                                   'median_CI': array([0., 1.])},\n",
      "              'Espirito Santo': {'mean_CI': [11.576126838361716,\n",
      "                                             21.123873161638286],\n",
      "                                 'median_CI': array([ 7.5, 16. ])},\n",
      "              'Goias': {'mean_CI': [35.15170758039637, 78.44829241960363],\n",
      "                        'median_CI': array([25., 49.])},\n",
      "              'Maranhao': {'mean_CI': [29.985825486976267, 72.21417451302374],\n",
      "                           'median_CI': array([16.5, 46. ])},\n",
      "              'Minas Gerais': {'mean_CI': [74.98269787598657,\n",
      "                                           105.81730212401344],\n",
      "                               'median_CI': array([ 68., 111.])},\n",
      "              'Para': {'mean_CI': [45.24491564383278, 107.4550843561672],\n",
      "                       'median_CI': array([26., 54.])},\n",
      "              'Pernambuco': {'mean_CI': [21.17687194138535, 31.32312805861465],\n",
      "                             'median_CI': array([21., 29.])},\n",
      "              'Piau': {'mean_CI': [21.37917805336479, 44.020821946635216],\n",
      "                       'median_CI': array([17.5, 24.5])},\n",
      "              'Rondonia': {'mean_CI': [7.437841838801645, 19.262158161198354],\n",
      "                           'median_CI': array([6.5, 9.5])},\n",
      "              'Roraima': {'mean_CI': [198.55379036815907, 343.2462096318409],\n",
      "                          'median_CI': array([130., 259.])},\n",
      "              'Santa Catarina': {'mean_CI': [12.55055958507197,\n",
      "                                             20.749440414928028],\n",
      "                                 'median_CI': array([10., 19.])},\n",
      "              'Sao Paulo': {'mean_CI': [60.885792186069445, 89.51420781393057],\n",
      "                            'median_CI': array([56., 87.])},\n",
      "              'Sergipe': {'mean_CI': [21.78151298373475, 33.21848701626525],\n",
      "                          'median_CI': array([19. , 33.5])},\n",
      "              'Tocantins': {'mean_CI': [16.951639127048292, 66.24836087295171],\n",
      "                            'median_CI': array([11.5, 27. ])}},\n",
      " 'January': {'Acre': {'mean_CI': [0.7702235159903026, 3.2297764840096974],\n",
      "                      'median_CI': array([0., 0.])},\n",
      "             'Alagoas': {'mean_CI': [33.94262524344172, 54.91451761370114],\n",
      "                         'median_CI': array([29., 49.])},\n",
      "             'Amapa': {'mean_CI': [6.464927379565313, 21.435072620434685],\n",
      "                       'median_CI': array([2. , 8.5])},\n",
      "             'Amazonas': {'mean_CI': [26.559251150082105, 126.54074884991789],\n",
      "                          'median_CI': array([16.5, 45. ])},\n",
      "             'Bahia': {'mean_CI': [155.42273189410685, 225.77726810589314],\n",
      "                       'median_CI': array([128.5, 232. ])},\n",
      "             'Ceara': {'mean_CI': [119.78798977710092, 192.2120102228991],\n",
      "                       'median_CI': array([ 97. , 207.2])},\n",
      "             'Distrito Federal': {'mean_CI': [0.19795006262129622,\n",
      "                                              0.6020499373787038],\n",
      "                                  'median_CI': array([0., 0.])},\n",
      "             'Espirito Santo': {'mean_CI': [8.2841006767222, 20.5158993232778],\n",
      "                                'median_CI': array([ 3.5, 11. ])},\n",
      "             'Goias': {'mean_CI': [27.98128869758557, 45.718711302414434],\n",
      "                       'median_CI': array([23., 36.])},\n",
      "             'Maranhao': {'mean_CI': [180.5231514808404, 274.7768485191596],\n",
      "                          'median_CI': array([159.5, 280. ])},\n",
      "             'Minas Gerais': {'mean_CI': [54.00692565454434, 78.89307434545566],\n",
      "                              'median_CI': array([37., 80.])},\n",
      "             'Para': {'mean_CI': [284.1400189288251, 556.3599810711748],\n",
      "                      'median_CI': array([167. , 414.5])},\n",
      "             'Pernambuco': {'mean_CI': [70.43787743835583, 116.66212256164417],\n",
      "                            'median_CI': array([ 48., 124.])},\n",
      "             'Piau': {'mean_CI': [56.263030051773555, 78.33696994822644],\n",
      "                      'median_CI': array([57., 85.])},\n",
      "             'Rondonia': {'mean_CI': [14.88045179724936, 32.619548202750636],\n",
      "                          'median_CI': array([ 5.5, 24. ])},\n",
      "             'Roraima': {'mean_CI': [174.82952614669284, 398.67047385330716],\n",
      "                         'median_CI': array([108., 302.])},\n",
      "             'Santa Catarina': {'mean_CI': [20.253682641210673,\n",
      "                                            30.746317358789327],\n",
      "                                'median_CI': array([14., 31.])},\n",
      "             'Sao Paulo': {'mean_CI': [34.02542191346259, 53.174578086537416],\n",
      "                           'median_CI': array([27. , 41.5])},\n",
      "             'Sergipe': {'mean_CI': [27.93074657566358, 54.86925342433642],\n",
      "                         'median_CI': array([24.5, 37.5])},\n",
      "             'Tocantins': {'mean_CI': [36.5831239151698, 60.9168760848302],\n",
      "                           'median_CI': array([34.5, 46.5])}},\n",
      " 'July': {'Acre': {'mean_CI': [69.0993225895734, 160.5006774104266],\n",
      "                   'median_CI': array([39. , 76.5])},\n",
      "          'Alagoas': {'mean_CI': [0.1590285062462673, 0.9409714937537328],\n",
      "                      'median_CI': array([0., 0.])},\n",
      "          'Amapa': {'mean_CI': [1.0689087163892808, 2.1310912836107194],\n",
      "                    'median_CI': array([0., 2.])},\n",
      "          'Amazonas': {'mean_CI': [206.38153227953245, 485.4184677204675],\n",
      "                       'median_CI': array([134.5, 241. ])},\n",
      "          'Bahia': {'mean_CI': [307.31764790598595, 428.8823520940141],\n",
      "                    'median_CI': array([265., 333.])},\n",
      "          'Ceara': {'mean_CI': [24.836009421895827, 49.06399057810418],\n",
      "                    'median_CI': array([14.  , 34.55])},\n",
      "          'Distrito Federal': {'mean_CI': [23.949914736923642,\n",
      "                                           36.550085263076355],\n",
      "                               'median_CI': array([20.5, 31. ])},\n",
      "          'Espirito Santo': {'mean_CI': [13.86863395198952, 20.831366048010484],\n",
      "                             'median_CI': array([10.5 , 18.05])},\n",
      "          'Goias': {'mean_CI': [416.79922684305103, 540.700773156949],\n",
      "                    'median_CI': array([360.5, 538. ])},\n",
      "          'Maranhao': {'mean_CI': [1028.7492622783086, 1544.2507377216914],\n",
      "                       'median_CI': array([1024. , 1285.5])},\n",
      "          'Minas Gerais': {'mean_CI': [434.20532313457767, 561.1946768654223],\n",
      "                           'median_CI': array([395.5, 584.5])},\n",
      "          'Para': {'mean_CI': [967.0435402555952, 1815.256459744405],\n",
      "                   'median_CI': array([661. , 976.5])},\n",
      "          'Pernambuco': {'mean_CI': [4.106519303651012, 7.093480696348987],\n",
      "                         'median_CI': array([2., 6.])},\n",
      "          'Piau': {'mean_CI': [459.8949630650503, 748.3050369349497],\n",
      "                   'median_CI': array([361., 643.])},\n",
      "          'Rondonia': {'mean_CI': [325.43281680533335, 549.7671831946667],\n",
      "                       'median_CI': array([192. , 440.5])},\n",
      "          'Roraima': {'mean_CI': [2.2346458787918477, 4.865354121208152],\n",
      "                      'median_CI': array([1. , 3.5])},\n",
      "          'Santa Catarina': {'mean_CI': [47.24716811965519, 139.3528318803448],\n",
      "                             'median_CI': array([31., 69.])},\n",
      "          'Sao Paulo': {'mean_CI': [327.2953049936912, 468.8046950063088],\n",
      "                        'median_CI': array([232.5, 479. ])},\n",
      "          'Sergipe': {'mean_CI': [-0.016386410451339914, 0.11638641045133992],\n",
      "                      'median_CI': array([0., 0.])},\n",
      "          'Tocantins': {'mean_CI': [980.2990237438227, 1384.1009762561775],\n",
      "                        'median_CI': array([ 906. , 1235.5])}},\n",
      " 'June': {'Acre': {'mean_CI': [6.3253517751009065, 19.374648224899094],\n",
      "                   'median_CI': array([1., 7.])},\n",
      "          'Alagoas': {'mean_CI': [0.5585901407037022, 1.4414098592962978],\n",
      "                      'median_CI': array([0., 1.])},\n",
      "          'Amapa': {'mean_CI': [0.6811608352277876, 1.8188391647722124],\n",
      "                    'median_CI': array([0. , 1.5])},\n",
      "          'Amazonas': {'mean_CI': [20.647130720903455, 41.15286927909654],\n",
      "                       'median_CI': array([ 9. , 27.5])},\n",
      "          'Bahia': {'mean_CI': [167.28372466025388, 234.21627533974612],\n",
      "                    'median_CI': array([140., 242.])},\n",
      "          'Ceara': {'mean_CI': [8.272788805557365, 16.227211194442635],\n",
      "                    'median_CI': array([ 3., 16.])},\n",
      "          'Distrito Federal': {'mean_CI': [6.039472527917967,\n",
      "                                           12.360527472082032],\n",
      "                               'median_CI': array([3. , 6.5])},\n",
      "          'Espirito Santo': {'mean_CI': [9.626250197997743, 16.673749802002256],\n",
      "                             'median_CI': array([ 6.5, 13.5])},\n",
      "          'Goias': {'mean_CI': [217.7952251217744, 275.7047748782256],\n",
      "                    'median_CI': array([212., 283.])},\n",
      "          'Maranhao': {'mean_CI': [364.69198597447405, 542.2080140255259],\n",
      "                       'median_CI': array([279. , 565.5])},\n",
      "          'Minas Gerais': {'mean_CI': [164.40063149924353, 219.49410534286173],\n",
      "                           'median_CI': array([144., 209.])},\n",
      "          'Para': {'mean_CI': [132.47931999162483, 247.6383270671987],\n",
      "                   'median_CI': array([ 94., 173.])},\n",
      "          'Pernambuco': {'mean_CI': [2.4874568942937243, 4.512543105706276],\n",
      "                         'median_CI': array([1.5, 4. ])},\n",
      "          'Piau': {'mean_CI': [145.5968181341717, 260.2031818658283],\n",
      "                   'median_CI': array([ 88. , 216.1])},\n",
      "          'Rondonia': {'mean_CI': [61.07866963222634, 96.52133036777366],\n",
      "                       'median_CI': array([38. , 96.5])},\n",
      "          'Roraima': {'mean_CI': [2.502408829822409, 5.897591170177591],\n",
      "                      'median_CI': array([1. , 4.5])},\n",
      "          'Santa Catarina': {'mean_CI': [22.213725690257682,\n",
      "                                         45.086274309742315],\n",
      "                             'median_CI': array([ 3. , 36.5])},\n",
      "          'Sao Paulo': {'mean_CI': [184.64695672185127, 289.95304327814875],\n",
      "                        'median_CI': array([137.5, 231. ])},\n",
      "          'Sergipe': {'mean_CI': [0.2535018477291424, 0.8464981522708577],\n",
      "                      'median_CI': array([0., 0.])},\n",
      "          'Tocantins': {'mean_CI': [545.9900995787845, 781.3099004212155],\n",
      "                        'median_CI': array([362.5, 795. ])}},\n",
      " 'March': {'Acre': {'mean_CI': [0.40840266022068905, 1.8915973397793109],\n",
      "                    'median_CI': array([0., 1.])},\n",
      "           'Alagoas': {'mean_CI': [22.635818557932954, 32.76418144206705],\n",
      "                       'median_CI': array([25.5, 32. ])},\n",
      "           'Amapa': {'mean_CI': [0.4319483154367951, 1.168051684563205],\n",
      "                     'median_CI': array([0., 1.])},\n",
      "           'Amazonas': {'mean_CI': [24.983975352122453, 47.81602464787754],\n",
      "                        'median_CI': array([11. , 47.5])},\n",
      "           'Bahia': {'mean_CI': [138.66424533013918, 201.63575466986083],\n",
      "                     'median_CI': array([140.5, 181. ])},\n",
      "           'Ceara': {'mean_CI': [5.928775485236002, 12.271224514763997],\n",
      "                     'median_CI': array([3. , 7.5])},\n",
      "           'Distrito Federal': {'mean_CI': [0.1304049279073506,\n",
      "                                            0.4695950720926494],\n",
      "                                'median_CI': array([0., 0.])},\n",
      "           'Espirito Santo': {'mean_CI': [7.981589847103018,\n",
      "                                          22.018410152896983],\n",
      "                              'median_CI': array([ 4.5, 11. ])},\n",
      "           'Goias': {'mean_CI': [35.050344405148266, 57.04965559485173],\n",
      "                     'median_CI': array([29., 53.])},\n",
      "           'Maranhao': {'mean_CI': [22.552100072904935, 39.947899927095065],\n",
      "                        'median_CI': array([16. , 32.5])},\n",
      "           'Minas Gerais': {'mean_CI': [66.83981712593324, 92.66018287406676],\n",
      "                            'median_CI': array([ 69., 105.])},\n",
      "           'Para': {'mean_CI': [21.466430195689988, 47.93356980431002],\n",
      "                    'median_CI': array([14., 28.])},\n",
      "           'Pernambuco': {'mean_CI': [17.60245524921536, 27.99754475078464],\n",
      "                          'median_CI': array([12., 32.])},\n",
      "           'Piau': {'mean_CI': [20.491081372478362, 35.10891862752164],\n",
      "                    'median_CI': array([14.5, 27. ])},\n",
      "           'Rondonia': {'mean_CI': [9.539690667321848, 14.160309332678152],\n",
      "                        'median_CI': array([ 9., 15.])},\n",
      "           'Roraima': {'mean_CI': [301.8995695631107, 517.1004304368893],\n",
      "                       'median_CI': array([179., 417.])},\n",
      "           'Santa Catarina': {'mean_CI': [21.621375601312547,\n",
      "                                          37.17862439868745],\n",
      "                              'median_CI': array([13., 33.])},\n",
      "           'Sao Paulo': {'mean_CI': [65.17097953696417, 85.52902046303582],\n",
      "                         'median_CI': array([70.5, 84. ])},\n",
      "           'Sergipe': {'mean_CI': [24.762690250001302, 37.837309749998695],\n",
      "                       'median_CI': array([23. , 41.1])},\n",
      "           'Tocantins': {'mean_CI': [31.45670118674703, 59.74329881325298],\n",
      "                         'median_CI': array([14.5, 58. ])}},\n",
      " 'May': {'Acre': {'mean_CI': [2.5316435748673864, 5.568356425132613],\n",
      "                  'median_CI': array([1. , 3.5])},\n",
      "         'Alagoas': {'mean_CI': [1.0672470839063075, 4.532752916093692],\n",
      "                     'median_CI': array([0., 1.])},\n",
      "         'Amapa': {'mean_CI': [0.35664780573159166, 0.9433521942684084],\n",
      "                   'median_CI': array([0. , 0.5])},\n",
      "         'Amazonas': {'mean_CI': [6.743103676036726, 15.156896323963274],\n",
      "                      'median_CI': array([ 3., 10.])},\n",
      "         'Bahia': {'mean_CI': [108.32699518715987, 156.0730048128401],\n",
      "                   'median_CI': array([102.5, 144. ])},\n",
      "         'Ceara': {'mean_CI': [3.16685929156038, 13.83314070843962],\n",
      "                   'median_CI': array([1. , 5.5])},\n",
      "         'Distrito Federal': {'mean_CI': [3.051257022667575, 6.548742977332425],\n",
      "                              'median_CI': array([2., 4.])},\n",
      "         'Espirito Santo': {'mean_CI': [5.576647448385952, 12.323352551614047],\n",
      "                            'median_CI': array([2. , 8.5])},\n",
      "         'Goias': {'mean_CI': [100.47846779955321, 139.3215322004468],\n",
      "                   'median_CI': array([ 96.5, 143. ])},\n",
      "         'Maranhao': {'mean_CI': [71.234015290901, 123.46598470909899],\n",
      "                      'median_CI': array([ 49.5, 108. ])},\n",
      "         'Minas Gerais': {'mean_CI': [95.12937839408752, 127.27062160591248],\n",
      "                          'median_CI': array([ 83. , 129.5])},\n",
      "         'Para': {'mean_CI': [24.587015455241453, 50.21298454475854],\n",
      "                  'median_CI': array([ 9., 42.])},\n",
      "         'Pernambuco': {'mean_CI': [3.9578981756260725, 9.042101824373928],\n",
      "                        'median_CI': array([1. , 6.5])},\n",
      "         'Piau': {'mean_CI': [41.56208128816364, 91.03791871183635],\n",
      "                  'median_CI': array([25., 60.])},\n",
      "         'Rondonia': {'mean_CI': [13.163316649809126, 22.736683350190873],\n",
      "                      'median_CI': array([ 8., 21.])},\n",
      "         'Roraima': {'mean_CI': [5.268677099075013, 34.33132290092499],\n",
      "                     'median_CI': array([2., 8.])},\n",
      "         'Santa Catarina': {'mean_CI': [15.017902498875078, 28.66630802744071],\n",
      "                            'median_CI': array([ 2., 33.])},\n",
      "         'Sao Paulo': {'mean_CI': [111.06057488662402, 181.33942511337597],\n",
      "                       'median_CI': array([ 78.5, 146. ])},\n",
      "         'Sergipe': {'mean_CI': [1.1259118677258586, 3.8740881322741414],\n",
      "                     'median_CI': array([0., 1.])},\n",
      "         'Tocantins': {'mean_CI': [212.95546268272665, 354.6445373172734],\n",
      "                       'median_CI': array([ 96. , 361.5])}},\n",
      " 'November': {'Acre': {'mean_CI': [24.4553410986439, 46.3446589013561],\n",
      "                       'median_CI': array([12.5, 39. ])},\n",
      "              'Alagoas': {'mean_CI': [24.668949226032225, 47.13105077396777],\n",
      "                          'median_CI': array([17., 28.])},\n",
      "              'Amapa': {'mean_CI': [462.5578524033352, 699.8421475966649],\n",
      "                        'median_CI': array([311.5, 804. ])},\n",
      "              'Amazonas': {'mean_CI': [251.64022772147888, 437.95977227852114],\n",
      "                           'median_CI': array([189., 354.])},\n",
      "              'Bahia': {'mean_CI': [527.8065054345595, 1033.0934945654406],\n",
      "                        'median_CI': array([263.5, 619.6])},\n",
      "              'Ceara': {'mean_CI': [1152.6854004289107, 1645.1145995710895],\n",
      "                        'median_CI': array([1003.5, 1561. ])},\n",
      "              'Distrito Federal': {'mean_CI': [0.3125238139494438,\n",
      "                                               0.9874761860505563],\n",
      "                                   'median_CI': array([0. , 0.5])},\n",
      "              'Espirito Santo': {'mean_CI': [11.916324603734006,\n",
      "                                             34.483675396265994],\n",
      "                                 'median_CI': array([ 4. , 14.5])},\n",
      "              'Goias': {'mean_CI': [98.82346921282615, 168.77653078717387],\n",
      "                        'median_CI': array([ 84. , 130.5])},\n",
      "              'Maranhao': {'mean_CI': [2701.7242648223564, 3637.4757351776434],\n",
      "                           'median_CI': array([2283.5, 3869. ])},\n",
      "              'Minas Gerais': {'mean_CI': [271.1966963618733,\n",
      "                                           519.5033036381267],\n",
      "                               'median_CI': array([153., 398.])},\n",
      "              'Para': {'mean_CI': [4759.42141850547, 6411.3785814945295],\n",
      "                       'median_CI': array([3688., 6981.])},\n",
      "              'Pernambuco': {'mean_CI': [248.8746029169646, 371.32539708303545],\n",
      "                             'median_CI': array([191., 372.])},\n",
      "              'Piau': {'mean_CI': [800.7819203991966, 1063.0180796008035],\n",
      "                       'median_CI': array([ 690., 1126.])},\n",
      "              'Rondonia': {'mean_CI': [199.46879495703666, 348.9312050429633],\n",
      "                           'median_CI': array([128. , 308.5])},\n",
      "              'Roraima': {'mean_CI': [86.29506490301154, 144.20493509698846],\n",
      "                          'median_CI': array([ 57.5, 133. ])},\n",
      "              'Santa Catarina': {'mean_CI': [83.01764746786381,\n",
      "                                             154.8823525321362],\n",
      "                                 'median_CI': array([53., 86.])},\n",
      "              'Sao Paulo': {'mean_CI': [83.83127275895768, 155.56872724104232],\n",
      "                            'median_CI': array([ 56., 108.])},\n",
      "              'Sergipe': {'mean_CI': [9.64450438739904, 19.15549561260096],\n",
      "                          'median_CI': array([ 6. , 12.5])},\n",
      "              'Tocantins': {'mean_CI': [196.72371950656827, 320.87628049343175],\n",
      "                            'median_CI': array([145. , 276.1])}},\n",
      " 'October': {'Acre': {'mean_CI': [331.915049475068, 543.3849505249319],\n",
      "                      'median_CI': array([287., 513.])},\n",
      "             'Alagoas': {'mean_CI': [17.068350478109778, 26.33164952189022],\n",
      "                         'median_CI': array([14.5, 23. ])},\n",
      "             'Amapa': {'mean_CI': [314.95661910917016, 453.64338089082986],\n",
      "                       'median_CI': array([329.5, 436.5])},\n",
      "             'Amazonas': {'mean_CI': [728.1680640703648, 1134.431935929635],\n",
      "                          'median_CI': array([663.5, 889. ])},\n",
      "             'Bahia': {'mean_CI': [2992.4092434715826, 4274.790756528418],\n",
      "                       'median_CI': array([2166., 4607.])},\n",
      "             'Ceara': {'mean_CI': [801.1555597857459, 1091.044440214254],\n",
      "                       'median_CI': array([ 658., 1059.])},\n",
      "             'Distrito Federal': {'mean_CI': [9.161618145941452,\n",
      "                                              18.13838185405855],\n",
      "                                  'median_CI': array([ 6. , 16.5])},\n",
      "             'Espirito Santo': {'mean_CI': [49.09034781751664,\n",
      "                                            91.33070481406233],\n",
      "                                'median_CI': array([37., 62.])},\n",
      "             'Goias': {'mean_CI': [935.1488324324746, 1283.2511675675255],\n",
      "                       'median_CI': array([ 824., 1253.])},\n",
      "             'Maranhao': {'mean_CI': [3682.1227918020572, 4781.177208197942],\n",
      "                          'median_CI': array([3228., 5244.])},\n",
      "             'Minas Gerais': {'mean_CI': [2182.2461924192557,\n",
      "                                          3235.7538075807443],\n",
      "                              'median_CI': array([1292., 3467.])},\n",
      "             'Para': {'mean_CI': [4632.253781399729, 5479.846218600272],\n",
      "                      'median_CI': array([4519.5, 5189. ])},\n",
      "             'Pernambuco': {'mean_CI': [321.07483025441775, 433.7251697455822],\n",
      "                            'median_CI': array([274. , 409.5])},\n",
      "             'Piau': {'mean_CI': [2131.9227368530564, 2659.7772631469434],\n",
      "                      'median_CI': array([1987., 2586.])},\n",
      "             'Rondonia': {'mean_CI': [1446.7911680892357, 2121.308831910764],\n",
      "                          'median_CI': array([1229. , 1953.5])},\n",
      "             'Roraima': {'mean_CI': [63.93320867304456, 102.36679132695545],\n",
      "                         'median_CI': array([57., 88.])},\n",
      "             'Santa Catarina': {'mean_CI': [85.3599297216131,\n",
      "                                            204.04007027838688],\n",
      "                                'median_CI': array([ 46.5, 102. ])},\n",
      "             'Sao Paulo': {'mean_CI': [261.7183751846019, 406.38162481539814],\n",
      "                           'median_CI': array([195., 341.])},\n",
      "             'Sergipe': {'mean_CI': [2.5583614363871368, 5.441638563612863],\n",
      "                         'median_CI': array([2., 3.])},\n",
      "             'Tocantins': {'mean_CI': [1609.001960819577, 2273.498039180423],\n",
      "                           'median_CI': array([1484.5, 2155. ])}},\n",
      " 'September': {'Acre': {'mean_CI': [1681.7505519144036, 2394.3494480855966],\n",
      "                        'median_CI': array([1807.5, 2272.5])},\n",
      "               'Alagoas': {'mean_CI': [6.699463490272773, 9.900536509727228],\n",
      "                           'median_CI': array([5., 8.])},\n",
      "               'Amapa': {'mean_CI': [80.29583038216612, 111.40416961783387],\n",
      "                         'median_CI': array([ 79. , 104.5])},\n",
      "               'Amazonas': {'mean_CI': [1503.0801831345032, 2382.419816865497],\n",
      "                            'median_CI': array([1146.5, 2499.5])},\n",
      "               'Bahia': {'mean_CI': [3880.128928432923, 5287.171071567076],\n",
      "                         'median_CI': array([3006., 5878.])},\n",
      "               'Ceara': {'mean_CI': [237.99000992379263, 352.90999007620735],\n",
      "                         'median_CI': array([177., 299.])},\n",
      "               'Distrito Federal': {'mean_CI': [41.975178532685995,\n",
      "                                                71.724821467314],\n",
      "                                    'median_CI': array([27., 65.])},\n",
      "               'Espirito Santo': {'mean_CI': [60.13395160066908,\n",
      "                                              97.46604839933092],\n",
      "                                  'median_CI': array([52.45, 70.5 ])},\n",
      "               'Goias': {'mean_CI': [1703.1985919275676, 2375.1014080724326],\n",
      "                         'median_CI': array([1562.5, 2259. ])},\n",
      "               'Maranhao': {'mean_CI': [4204.416639278353, 6095.283360721648],\n",
      "                            'median_CI': array([3880., 5255.])},\n",
      "               'Minas Gerais': {'mean_CI': [2756.5091631707182,\n",
      "                                            3732.9908368292818],\n",
      "                                'median_CI': array([2389.5, 3336.5])},\n",
      "               'Para': {'mean_CI': [5331.752295016645, 8461.147704983354],\n",
      "                        'median_CI': array([5609. , 7370.5])},\n",
      "               'Pernambuco': {'mean_CI': [149.99580009974284,\n",
      "                                          214.20419990025715],\n",
      "                              'median_CI': array([147. , 177.1])},\n",
      "               'Piau': {'mean_CI': [2414.3373712105354, 3253.262628789465],\n",
      "                        'median_CI': array([2196., 3136.])},\n",
      "               'Rondonia': {'mean_CI': [4139.97557708613, 6280.324422913869],\n",
      "                            'median_CI': array([3062., 5638.])},\n",
      "               'Roraima': {'mean_CI': [25.355752693135237, 44.44424730686476],\n",
      "                           'median_CI': array([14.5, 44. ])},\n",
      "               'Santa Catarina': {'mean_CI': [251.0820000393573,\n",
      "                                              447.61799996064275],\n",
      "                                  'median_CI': array([172. , 416.5])},\n",
      "               'Sao Paulo': {'mean_CI': [585.5569773036776, 946.3430226963225],\n",
      "                             'median_CI': array([492.5, 809. ])},\n",
      "               'Sergipe': {'mean_CI': [0.45808539321873587, 1.1419146067812642],\n",
      "                           'median_CI': array([0., 1.])},\n",
      "               'Tocantins': {'mean_CI': [3847.0546098158884, 5331.045390184112],\n",
      "                             'median_CI': array([3382.5, 5149. ])}}}\n"
     ]
    }
   ],
   "source": [
    "#DONT CHANGE THIS. WE USE IT TO MAKE THE OUTPUT LEGIBLE FOR GRADING\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "pp.pprint(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given Test for the mean confidence intervals\n",
    "\n",
    "rounded_mean_CI = [round(x, 2) for x in months['April']['Acre']['mean_CI']]\n",
    "assert rounded_mean_CI == [0.76, 3.34], 'somethings wrong in the mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given test for the median confidence intervals. \n",
    "#Your code is probably correct if it passes this test, but since bootstrapping the medain is a stochastic process\n",
    "#you may have this test fail. If it fails, run it a few times. \n",
    "#If it continues to fail, your code is probably incorrect.\n",
    "\n",
    "low_median_CI = months['April']['Acre']['median_CI'][0]\n",
    "high_median_CI = months['April']['Acre']['median_CI'][1]\n",
    "assert -1 <= low_median_CI <= 1 and 0 <= high_median_CI <= 3, 'somethings wrong in the median'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E: Where Do The Firefighters Go?\n",
    "Now, we'll determine which state the Brazilian government should assign it's fire fighters to. For each month of the year, you should perform the folllowing selection process:\n",
    "1. Find the state with the highest CI for the median for this month (it's easiest and ok to just use the upper bound here). \n",
    "2. Find any states that have a median CI that overlaps with the highest CI foud in step 1. If no states overlap with the highest CI found in step 1, then use that stat. \n",
    "3. If overlapping confidence intervals are found on the median, we'll use the CI for the mean to break ties.\n",
    "4. Out of the states with overlapping CIs for median (every state in part 3), find the state with the highest mean CI. \n",
    "5. Determine if any of the states from part 3 have a mean CI that overlaps with the state found in step 4. \n",
    "6. If no state overlap with the state found in part 4, then just use that state. If other states have overlapping mean CIs too, then we'll split up the firefighters and assign some of them to every state that has both an overlapping median and mean CI with the state that has the highest median CI.\n",
    "\n",
    "Once you've used the selection process above, use a markdown table to display a list of each state that recieves  some of the firefighters for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month: January\n",
      "\n",
      "250.0 allocated to: Roraima\n",
      "\n",
      "250.0 allocated to: Para\n",
      "\n",
      "Month: February\n",
      "\n",
      "500 allocated to: Roraima\n",
      "\n",
      "Month: March\n",
      "\n",
      "500 allocated to: Roraima\n",
      "\n",
      "Month: April\n",
      "\n",
      "100.0 allocated to: Minas Gerais\n",
      "\n",
      "100.0 allocated to: Sao Paulo\n",
      "\n",
      "100.0 allocated to: Tocantins\n",
      "\n",
      "100.0 allocated to: Roraima\n",
      "\n",
      "100.0 allocated to: Bahia\n",
      "\n",
      "Month: May\n",
      "\n",
      "500 allocated to: Tocantins\n",
      "\n",
      "Month: June\n",
      "\n",
      "500 allocated to: Tocantins\n",
      "\n",
      "Month: July\n",
      "\n",
      "250.0 allocated to: Tocantins\n",
      "\n",
      "250.0 allocated to: Maranhao\n",
      "\n",
      "Month: August\n",
      "\n",
      "500 allocated to: Para\n",
      "\n",
      "Month: September\n",
      "\n",
      "250.0 allocated to: Rondonia\n",
      "\n",
      "250.0 allocated to: Para\n",
      "\n",
      "Month: October\n",
      "\n",
      "250.0 allocated to: Maranhao\n",
      "\n",
      "250.0 allocated to: Para\n",
      "\n",
      "Month: November\n",
      "\n",
      "500 allocated to: Para\n",
      "\n",
      "Month: December\n",
      "\n",
      "500 allocated to: Para\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loop through each month\n",
    "for m in mnths:\n",
    "    print (\"Month:\", m)\n",
    "    print (\"\")\n",
    "    high_CI = months[m]['Acre']['median_CI'][1] #set the highest to first CI\n",
    "    low_CI = months[m]['Acre']['median_CI'][0] #set the lowest to first CI\n",
    "    name = \"Acre\"\n",
    "    firefighters = 500 #number of firefighters to be allocated\n",
    "    \n",
    "    #Step 1: loop through each state and find the highest median\n",
    "    for s in states: \n",
    "        if months[m][s]['median_CI'][1] > high_CI:\n",
    "            high_CI = months[m][s]['median_CI'][1] #reset high\n",
    "            low_CI = months[m][s]['median_CI'][0] #reset low\n",
    "            name = s #reset name\n",
    "    \n",
    "    list_overlap_median = []\n",
    "    \n",
    "    #Step 2: find overlapping CI's\n",
    "    for s in states:\n",
    "        if s != name:\n",
    "            if months[m][s]['median_CI'][1] > low_CI:\n",
    "                list_overlap_median.append(s)\n",
    "\n",
    "    count = 0\n",
    "    while count < 500:\n",
    "        \n",
    "        #Step 3: If list is empty, assign firefighters to state in Step 1, else add step 1 state to list\n",
    "        if len(list_overlap_median) == 0:\n",
    "            print (firefighters, \"allocated to:\", name)\n",
    "            print (\"\")\n",
    "            count += 500\n",
    "            break\n",
    "        else:\n",
    "            list_overlap_median.append(name)\n",
    "            \n",
    "        #Step 4: Find the highest mean CI\n",
    "        \n",
    "        #set highest and lowest to the first name in the list\n",
    "        name = list_overlap_median[0]\n",
    "        high_mean = months[m][name]['mean_CI'][1]\n",
    "        low_mean = months[m][name]['mean_CI'][0]\n",
    "        \n",
    "        list_overlap_mean = []\n",
    "        \n",
    "        #loop through the rest of the list to find the highest mean CI\n",
    "        for s in list_overlap_median:\n",
    "            if months[m][s]['mean_CI'][1] > high_mean:\n",
    "                name = s\n",
    "                high_mean = months[m][s]['mean_CI'][1]\n",
    "                low_mean = months[m][s]['mean_CI'][0]\n",
    "                \n",
    "        #Step 5: find overlapping CI's\n",
    "        for s in list_overlap_median:\n",
    "            if s != name:\n",
    "                if months[m][s]['mean_CI'][1] > low_mean:\n",
    "                    list_overlap_mean.append(s)\n",
    "        \n",
    "        #Step 6: Allocate firefighters\n",
    "        if len(list_overlap_mean) == 0:\n",
    "            print (firefighters, \"allocated to:\", name)\n",
    "            print (\"\")\n",
    "            count += 500\n",
    "            break\n",
    "        else:\n",
    "            list_overlap_mean.append(name)\n",
    "            n = len(list_overlap_mean)\n",
    "            ffs = firefighters/n\n",
    "            for s in list_overlap_mean:\n",
    "                print (ffs, \"allocated to:\", s)\n",
    "                print (\"\")\n",
    "                count += ffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the allocated firefighters, by month and by state, is presented below:\n",
    "\n",
    "\n",
    "| Month  / State    | January | February | March | April | May | June | July | August | Sept. | Oct. | Nov. | Dec. |\n",
    "|:------------------|---------|----------|-------|-------|-----|------|------|--------|-------|------|------|-----:|\n",
    "| Acre              | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Alagoas           | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Amapa             | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Amazonas          | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Bahia             | -       | -        | -     | 100   | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Ceara             | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Distrito Federal  | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Espirito Santo    | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Goias             | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Maranhao          | -       | -        | -     | -     | -   | -    | 250  | -      | -     | 250  | -    | -    |\n",
    "| Minas Gerais      | -       | -        | -     | 100   | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Para              | 250     | -        | -     | -     | -   | -    | -    | 500    | 250   | 250  | 500  | 500  |\n",
    "| Pernambuco        | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Piau              | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Rondonia          | -       | -        | -     | -     | -   | -    | -    | -      | 250   | -    | -    | -    |\n",
    "| Roraima           | 250     | 500      | 500   | 100   | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Santa Catarina    | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Sao Paulo         | -       | -        | -     | 100   | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Sergipe           | -       | -        | -     | -     | -   | -    | -    | -      | -     | -    | -    | -    |\n",
    "| Tocantins         | -       | -        | -     | 100   | 500 | 500  | 250  | -      | -     | -    | -    | -    |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
